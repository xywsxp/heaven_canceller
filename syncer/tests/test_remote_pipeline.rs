use chrono::NaiveDate;
use std::collections::HashMap;
use syncer::config::RemoteConfig;
use syncer::extractor::ClickHouseExtractor;
use syncer::parquet_helper::ParquetHelper;
use syncer::pipeline::RemotePipeline;
use tempfile::tempdir;
use utils::clickhouse_client::ClickHouseClient;

/// è¾…åŠ©å‡½æ•°ï¼šåˆ›å»ºä¸´æ—¶æµ‹è¯•è¡¨
async fn create_test_table(table_name: &str, source_table: &str) -> Result<(), Box<dyn std::error::Error>> {
    let client = ClickHouseClient::instance().client();
    
    // åˆ é™¤å·²å­˜åœ¨çš„è¡¨
    let drop_sql = format!("DROP TABLE IF EXISTS {}", table_name);
    client.query(&drop_sql).execute().await?;
    
    // åˆ›å»ºæ–°è¡¨ï¼ˆå¤åˆ¶æºè¡¨ç»“æ„ï¼‰
    let create_sql = format!("CREATE TABLE {} AS {}", table_name, source_table);
    client.query(&create_sql).execute().await?;
    
    println!("  âœ“ Created test table: {}", table_name);
    Ok(())
}

/// è¾…åŠ©å‡½æ•°ï¼šåˆ é™¤æµ‹è¯•è¡¨
async fn drop_test_table(table_name: &str) -> Result<(), Box<dyn std::error::Error>> {
    let client = ClickHouseClient::instance().client();
    let drop_sql = format!("DROP TABLE IF EXISTS {}", table_name);
    client.query(&drop_sql).execute().await?;
    println!("  âœ“ Dropped test table: {}", table_name);
    Ok(())
}

/// è¾…åŠ©å‡½æ•°ï¼šæŸ¥è¯¢è¡¨è¡Œæ•°
async fn count_table_rows(table_name: &str) -> Result<u64, Box<dyn std::error::Error>> {
    let client = ClickHouseClient::instance().client();
    let count_sql = format!("SELECT count() FROM {}", table_name);
    
    let count: u64 = client
        .query(&count_sql)
        .fetch_one::<u64>()
        .await?;
    
    Ok(count)
}

#[tokio::test]
#[ignore = "integration test, requires ClickHouse"]
async fn test_remote_pipeline_integration() {
    // å‡†å¤‡ï¼šåˆ›å»ºä¸´æ—¶æµ‹è¯•è¡¨
    let test_table = "pumpfun_trade_event_v2_remote_test_tmp";
    
    println!("ğŸ”§ Setting up test table...");
    create_test_table(test_table, "pumpfun_trade_event_v2")
        .await
        .expect("Failed to create test table");
    
    // å‡†å¤‡ï¼šå…ˆåˆ›å»ºä¸€äº› Parquet æ–‡ä»¶
    let temp_dir = tempdir().unwrap();
    let storage_path = temp_dir.path().to_path_buf();
    
    println!("ğŸ“¦ Preparing test data...");
    
    // 1. æå–å‡ å¤©çš„æ•°æ®å¹¶å†™å…¥ Parquet
    let extractor = ClickHouseExtractor::new();
    let parquet_helper = ParquetHelper::new();
    
    let dates = vec![
        NaiveDate::from_ymd_opt(2025, 10, 1).unwrap(),
        NaiveDate::from_ymd_opt(2025, 10, 2).unwrap(),
    ];
    
    let mut expected_rows = 0u64;
    
    for date in &dates {
        let batch = extractor
            .extract_daily_events("pumpfun_trade_event_v2", "PumpfunTradeEventV2", *date)
            .await
            .expect("Failed to extract");
        
        expected_rows += batch.num_rows() as u64;
        
        parquet_helper
            .write_daily_parquet(
                "pumpfun_trade_event_v2",
                *date,
                batch,
                &storage_path,
            )
            .await
            .expect("Failed to write parquet");
        
        println!("  âœ“ Created parquet for {}", date);
    }
    
    println!("âœ“ Test data prepared (expected {} rows)\n", expected_rows);
    
    // 2. é…ç½® RemotePipeline
    let config = RemoteConfig {
        remote_storage_path: storage_path.clone(),
        import_mappings: [
            ("pumpfun_trade_event_v2".to_string(), test_table.to_string()),
        ]
        .into_iter()
        .collect(),
        table_event_mappings: [
            ("pumpfun_trade_event_v2".to_string(), "PumpfunTradeEventV2".to_string()),
        ]
        .into_iter()
        .collect(),
    };
    
    // 3. è¿è¡Œ RemotePipeline
    let pipeline = RemotePipeline::new(config);
    
    let result = pipeline.run().await;
    
    match result {
        Ok(_) => {
            println!("âœ… Remote pipeline completed successfully");
            
            // éªŒè¯å¯¼å…¥çš„è¡Œæ•°
            let actual_rows = count_table_rows(test_table).await.expect("Failed to count rows");
            println!("   Expected: {} rows, Actual: {} rows", expected_rows, actual_rows);
            
            assert_eq!(
                expected_rows, actual_rows,
                "Imported rows should match expected"
            );
        }
        Err(e) => {
            eprintln!("âœ— Remote pipeline failed: {}", e);
            panic!("Pipeline failed: {}", e);
        }
    }
    
    // æ¸…ç†ï¼šåˆ é™¤æµ‹è¯•è¡¨
    println!("\nğŸ§¹ Cleaning up...");
    drop_test_table(test_table).await.expect("Failed to drop test table");
}

#[tokio::test]
#[ignore = "integration test, requires ClickHouse"]
async fn test_remote_pipeline_multiple_tables() {
    let test_table_1 = "pumpfun_trade_event_v2_remote_test_tmp";
    let test_table_2 = "pumpfun_create_event_v2_remote_test_tmp";
    
    println!("ğŸ”§ Setting up test tables...");
    create_test_table(test_table_1, "pumpfun_trade_event_v2")
        .await
        .expect("Failed to create test table 1");
    create_test_table(test_table_2, "pumpfun_create_event_v2")
        .await
        .expect("Failed to create test table 2");
    
    let temp_dir = tempdir().unwrap();
    let storage_path = temp_dir.path().to_path_buf();
    
    println!("ğŸ“¦ Preparing test data for multiple tables...");
    
    let extractor = ClickHouseExtractor::new();
    let parquet_helper = ParquetHelper::new();
    let date = NaiveDate::from_ymd_opt(2025, 10, 1).unwrap();
    
    // å‡†å¤‡ä¸¤ä¸ªè¡¨çš„æ•°æ®
    let tables = vec![
        ("pumpfun_trade_event_v2", "PumpfunTradeEventV2"),
        ("pumpfun_create_event_v2", "PumpfunCreateEventV2"),
    ];
    
    for (table, event_type) in &tables {
        let batch = extractor
            .extract_daily_events(table, event_type, date)
            .await
            .expect("Failed to extract");
        
        parquet_helper
            .write_daily_parquet(table, date, batch, &storage_path)
            .await
            .expect("Failed to write");
        
        println!("  âœ“ Created parquet for {}", table);
    }
    
    println!("âœ“ Test data prepared\n");
    
    // é…ç½®å¯¼å…¥
    let config = RemoteConfig {
        remote_storage_path: storage_path,
        import_mappings: [
            ("pumpfun_trade_event_v2".to_string(), test_table_1.to_string()),
            ("pumpfun_create_event_v2".to_string(), test_table_2.to_string()),
        ]
        .into_iter()
        .collect(),
        table_event_mappings: [
            ("pumpfun_trade_event_v2".to_string(), "PumpfunTradeEventV2".to_string()),
            ("pumpfun_create_event_v2".to_string(), "PumpfunCreateEventV2".to_string()),
        ]
        .into_iter()
        .collect(),
    };
    
    let pipeline = RemotePipeline::new(config);
    
    match pipeline.run().await {
        Ok(_) => {
            println!("âœ… Multiple tables import completed");
            
            let rows1 = count_table_rows(test_table_1).await.expect("Failed to count");
            let rows2 = count_table_rows(test_table_2).await.expect("Failed to count");
            
            println!("   Table 1: {} rows", rows1);
            println!("   Table 2: {} rows", rows2);
            
            assert!(rows1 > 0, "Table 1 should have rows");
            assert!(rows2 > 0, "Table 2 should have rows");
        }
        Err(e) => {
            eprintln!("âœ— Failed: {}", e);
            panic!("Pipeline failed: {}", e);
        }
    }
    
    // æ¸…ç†
    println!("\nğŸ§¹ Cleaning up...");
    drop_test_table(test_table_1).await.ok();
    drop_test_table(test_table_2).await.ok();
}

#[tokio::test]
async fn test_remote_pipeline_empty_folder() {
    // æµ‹è¯•ç©ºæ–‡ä»¶å¤¹çš„å¤„ç†
    let temp_dir = tempdir().unwrap();
    let storage_path = temp_dir.path().to_path_buf();
    
    // åˆ›å»ºç©ºæ–‡ä»¶å¤¹
    std::fs::create_dir_all(storage_path.join("empty_folder")).unwrap();
    
    let config = RemoteConfig {
        remote_storage_path: storage_path,
        import_mappings: [
            ("empty_folder".to_string(), "test_table".to_string()),
        ]
        .into_iter()
        .collect(),
        table_event_mappings: [
            ("empty_folder".to_string(), "PumpfunTradeEventV2".to_string()),
        ]
        .into_iter()
        .collect(),
    };
    
    let pipeline = RemotePipeline::new(config);
    
    let result = pipeline.run().await;
    
    // åº”è¯¥æˆåŠŸä½†è·³è¿‡ç©ºæ–‡ä»¶å¤¹
    assert!(result.is_ok(), "Should handle empty folder gracefully");
    println!("âœ“ Empty folder handled correctly");
}

#[tokio::test]
async fn test_remote_pipeline_missing_folder() {
    // æµ‹è¯•ä¸å­˜åœ¨çš„æ–‡ä»¶å¤¹
    let temp_dir = tempdir().unwrap();
    
    let config = RemoteConfig {
        remote_storage_path: temp_dir.path().to_path_buf(),
        import_mappings: [
            ("nonexistent_folder".to_string(), "test_table".to_string()),
        ]
        .into_iter()
        .collect(),
        table_event_mappings: [
            ("nonexistent_folder".to_string(), "PumpfunTradeEventV2".to_string()),
        ]
        .into_iter()
        .collect(),
    };
    
    let pipeline = RemotePipeline::new(config);
    
    let result = pipeline.run().await;
    
    // åº”è¯¥æˆåŠŸä½†è·³è¿‡ä¸å­˜åœ¨çš„æ–‡ä»¶å¤¹
    assert!(result.is_ok(), "Should handle missing folder gracefully");
    println!("âœ“ Missing folder handled correctly");
}

#[tokio::test]
async fn test_remote_pipeline_missing_event_type() {
    let temp_dir = tempdir().unwrap();
    
    let config = RemoteConfig {
        remote_storage_path: temp_dir.path().to_path_buf(),
        import_mappings: [
            ("test_folder".to_string(), "test_table".to_string()),
        ]
        .into_iter()
        .collect(),
        table_event_mappings: HashMap::new(), // æ²¡æœ‰äº‹ä»¶ç±»å‹æ˜ å°„
    };
    
    let pipeline = RemotePipeline::new(config);
    
    let result = pipeline.run().await;
    
    assert!(result.is_err(), "Should fail when event type mapping is missing");
    
    if let Err(e) = result {
        let error_msg = e.to_string();
        assert!(
            error_msg.contains("Event type not found"),
            "Error should mention missing event type: {}",
            error_msg
        );
        println!("âœ“ Correctly validated event type mapping: {}", error_msg);
    }
}

#[tokio::test]
#[ignore = "integration test, requires ClickHouse"]
async fn test_remote_pipeline_file_ordering() {
    // æµ‹è¯•æ–‡ä»¶æŒ‰æ—¥æœŸé¡ºåºå¤„ç†
    let test_table = "pumpfun_trade_event_v2_remote_test_tmp";
    
    println!("ğŸ”§ Setting up test table...");
    create_test_table(test_table, "pumpfun_trade_event_v2")
        .await
        .expect("Failed to create test table");
    
    let temp_dir = tempdir().unwrap();
    let storage_path = temp_dir.path().to_path_buf();
    
    println!("ğŸ“¦ Creating files in random order...");
    
    let extractor = ClickHouseExtractor::new();
    let parquet_helper = ParquetHelper::new();
    
    // å€’åºåˆ›å»ºæ–‡ä»¶ï¼ˆæµ‹è¯•æ’åºï¼‰
    let dates = vec![
        NaiveDate::from_ymd_opt(2025, 10, 3).unwrap(),
        NaiveDate::from_ymd_opt(2025, 10, 1).unwrap(),
        NaiveDate::from_ymd_opt(2025, 10, 2).unwrap(),
    ];
    
    for date in &dates {
        let batch = extractor
            .extract_daily_events("pumpfun_trade_event_v2", "PumpfunTradeEventV2", *date)
            .await
            .expect("Failed to extract");
        
        parquet_helper
            .write_daily_parquet("pumpfun_trade_event_v2", *date, batch, &storage_path)
            .await
            .expect("Failed to write");
        
        println!("  âœ“ Created: {}", date);
    }
    
    println!("âœ“ Files created (should be sorted during import)\n");
    
    let config = RemoteConfig {
        remote_storage_path: storage_path,
        import_mappings: [
            ("pumpfun_trade_event_v2".to_string(), test_table.to_string()),
        ]
        .into_iter()
        .collect(),
        table_event_mappings: [
            ("pumpfun_trade_event_v2".to_string(), "PumpfunTradeEventV2".to_string()),
        ]
        .into_iter()
        .collect(),
    };
    
    let pipeline = RemotePipeline::new(config);
    
    match pipeline.run().await {
        Ok(_) => {
            println!("âœ… Files processed in sorted order");
            let rows = count_table_rows(test_table).await.expect("Failed to count");
            println!("   Total imported: {} rows", rows);
            assert!(rows > 0, "Should have imported rows");
        }
        Err(e) => {
            eprintln!("âœ— Failed: {}", e);
            panic!("Pipeline failed: {}", e);
        }
    }
    
    // æ¸…ç†
    println!("\nğŸ§¹ Cleaning up...");
    drop_test_table(test_table).await.ok();
}
